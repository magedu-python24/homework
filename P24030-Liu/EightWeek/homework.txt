如何设计一个日志分析系统,分析可能遇到问题和解决方法
概述
	生成中会生成大量的系统日志、应用程序日志、安全日志等等日志,通过对日志的分析可以了解服务器的负载、健康状况,可以分析客户的分布情况、客户的行为,甚至基于这些分析可以做出预测。
	一般采集流程
		日志产出 -> 采集(Logstash、Flume、Scribe) -> 存储 -> 分析 -> 存储(数据库、NoSQL) ->
	可视化
	开源实时日志分析ELK平台
	Logstash收集日志,并存放到ElasticSearch集群中,Kibana则从ES集群中查询数据生成图表,返回浏览器端


数据提取
	半结构化数据
		日志是半结构化数据,是有组织的,有格式的数据。可以分割成行和列,就可以当做表理解和处理了,当然也可以分析里面的数据。
	文本数据
		日志是文本文件,需要依赖文件IO、字符串操作、正则表达式等技术。通过这些技术就能够把日志中需要的数据提取出来。
	提取文本数据的方式
	
	空格分割
		缺点:数据并没有按照业务分割好,比如时间就被分开了,URL相关的也被分开了,所以,定义的时候不选用这种在filed中出现的字符就可以省很多事,例如使用'\x01'这个不可见的ASCII,print('\x01')试一试
	能否依旧是空格分割,但是遇到双引号、中括号特殊处理一下?
		思路:先按照空格切分,然后一个个字符迭代,但如果发现是 [ 或者 ",则就不判断是否有空格,直到 ] 或者" 结尾,这个区间获取的就是时间等数据。
	正则表达式提取
		按需编写正则表达式,可以根据自己的业务提取到想要的数据
		异常处理:日志中不免会出现一些不匹配的行,需要处理。可以使用re.match方法,有可能匹配不。增加一个判断


数据分发
	生产者消费者模型:对于一个监控系统,需要处理很多数据,包括日志。对其中已有数据的采集、分析。
被监控对象就是数据的生产者producer,数据的处理程序就是数据的消费者consumer。生产者消费者传统模型传统的生产者消费者模型,生产者生产,消费者消费。但这种模型有些问题开发的代码耦合太高,如果生成规模扩大,不易扩展,生产和消费的速度很难匹配等。

	解决办法:队列queue
		日志生产者往往会部署好几个程序,日志产生的也很多,而消费者也会有多个程序,去提取日志分析处理。数据的生产是不稳定的!可能会造成短时间数据的“潮涌”,需要缓冲。消费者消费能力不一样,有快有慢,消费者可以自己决定消费缓冲区中的数据。单机可以使用标准库queue模块的类来构建进程内的队列,满足多个线程间的生产消费需要。大型系统可以使用第三方消息中间件:RabbitMQ、RocketMQ、Kafka
	
		


